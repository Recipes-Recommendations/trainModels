{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfEU8fY0PqjJ"
      },
      "source": [
        "# Install and import dependencies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BLktHim-LSze"
      },
      "source": [
        "Installs dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kw2svCMZdWxZ"
      },
      "outputs": [],
      "source": [
        "!pip install accelerate bitsandbytes datasets loralib\n",
        "!pip install sentence-transformers transformers[torch] tqdm s3fs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0PFQg_2LVUf"
      },
      "source": [
        "Imports necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vFqFXRRadciL"
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset\n",
        "from huggingface_hub import notebook_login\n",
        "from peft import get_peft_model, LoraConfig, PeftConfig, PeftModel\n",
        "from transformers import AutoTokenizer\n",
        "from sentence_transformers import SentenceTransformer, SentenceTransformerTrainer\n",
        "from sentence_transformers.losses import ContrastiveLoss, CosineSimilarityLoss\n",
        "from sentence_transformers.training_args import SentenceTransformerTrainingArguments\n",
        "from tqdm import tqdm\n",
        "from zipfile import ZipFile\n",
        "import bitsandbytes as bnb\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import s3fs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDcyvHvUV9Yw"
      },
      "source": [
        "# Create Global Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EAx-5LWMWATo"
      },
      "outputs": [],
      "source": [
        "ORIGINAL_DATASET_S3_PATH = \"s3://path-to-original-dataset-zip-file\"\n",
        "RESULTS_S3_PATHS = [\n",
        "    \"s3://path-to-parquet-files-with-results-1\",\n",
        "    \"s3://path-to-parquet-files-with-results-2\"\n",
        "]\n",
        "DATASET_LOCAL_DISK_PATH = \"dataset.hf\"\n",
        "BASE_MODEL_ID = \"sentence-transformers/all-mpnet-base-v2\"\n",
        "HF_MODEL_ID = \"carlosalvarezg/all-mpnet-base-v2\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7vsjvZQPw_r"
      },
      "source": [
        "# Create Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GugrnsI3decY"
      },
      "outputs": [],
      "source": [
        "def read_csv_from_zip_s3(zip_file_path: str) -> pd.Dataframe:\n",
        "    \"\"\"\n",
        "    Read a CSV file from a zip file stored in S3.\n",
        "\n",
        "    Args:\n",
        "    zip_file_path (str): Full S3 path to the zip file (e.g., 's3://bucket/path/file.zip')\n",
        "\n",
        "    Returns:\n",
        "    pandas.DataFrame: The contents of the CSV file\n",
        "    \"\"\"\n",
        "    # Initialize S3 filesystem\n",
        "    fs = s3fs.S3FileSystem()\n",
        "\n",
        "    try:\n",
        "        # Read the zip file from S3\n",
        "        with fs.open(zip_file_path, 'rb') as zip_file:\n",
        "            # Create a pandas dataframe from the zip file\n",
        "            df = pd.read_csv(zip_file, compression=\"zip\")\n",
        "        return df\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading CSV from zip in S3: {str(e)}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yxHt6gu-dhpt"
      },
      "outputs": [],
      "source": [
        "def read_parquet_from_s3(s3_path: str) -> pd.Dataframe:\n",
        "  \"\"\"\n",
        "  Read a Parquet file from S3.\n",
        "\n",
        "  Args:\n",
        "  s3_path (str): Full S3 path to the Parquet file (e.g., 's3://bucket/path/file.parquet')\n",
        "\n",
        "  Returns:\n",
        "  pandas.DataFrame: The contents of the Parquet file\n",
        "  \"\"\"\n",
        "    # Initialize S3 filesystem\n",
        "    fs = s3fs.S3FileSystem()\n",
        "\n",
        "    try:\n",
        "        # Read the Parquet file directly using s3fs\n",
        "        df = pd.read_parquet(s3_path, filesystem=fs)\n",
        "\n",
        "        print(f\"Successfully read the Parquet file. Shape: {df.shape}\")\n",
        "        return df\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading Parquet file from S3: {str(e)}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fg29HRg_P0rJ"
      },
      "source": [
        "# Ceate Unified Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hagQ4Tl-NiVf"
      },
      "source": [
        "Uses original recipes_data.csv.zip file and dataset containing pairs of similar and different indices to create a unified dataset of similar and different titles and ingredients. The first columns contains titles of recipes, the second columns contains titles and ingredients of similar and different recipes. The third columns contains a label, which is 1 if the recipes in the current row are similar, and it's 0 if the recipes in the current row are different."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wD48EFTaQQar"
      },
      "source": [
        "Read original recipes_data.csv.zip file from S3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZFNmzxgedftD"
      },
      "outputs": [],
      "source": [
        "all_data = read_csv_from_zip_s3(ORIGINAL_DATASET_S3_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CPMTVkpQR1y"
      },
      "source": [
        "Read results from PySpark script"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wNwE-Fkjdjiz"
      },
      "outputs": [],
      "source": [
        "samples = [read_parquet_from_s3(s3_path) for s3_path in RESULTS_S3_PATHS]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4DQfmCg7QU7s"
      },
      "source": [
        "PySpark and Pandas process strings differently. This sometimes results in titles not being parsed correctly when they're written by PySpark into Parquet files and then read by Pandas. Therefore, we go thorough all the recipe titles in both the original dataset and the results and find any recipes in the results that are not in the original dataset. Then we add their indices in indices_to_remove. This usually filter out around 200 indices (out of 2.23 million)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fw_sbkUxdlX8"
      },
      "outputs": [],
      "source": [
        "title_counts = {}\n",
        "for title in tqdm(all_data[\"title\"], total=len(all_data)):\n",
        "    title = title.replace('\"', \"\").lower()\n",
        "    if title in title_counts:\n",
        "        title_counts[title] += 1\n",
        "    else:\n",
        "        title_counts[title] = 1\n",
        "\n",
        "indices_to_remove = [set() for _ in range(len(samples))]\n",
        "for index1, sample in tqdm(enumerate(samples), total=len(samples)):\n",
        "    for index2, title in enumerate(sample[\"title\"]):\n",
        "        title = title.replace('\"', \"\").lower()\n",
        "        if title in title_counts:\n",
        "            title_counts[title] -= 1\n",
        "            if title_counts[title] == 0:\n",
        "                title_counts.pop(title)\n",
        "        else:\n",
        "            title_counts[title] = -1\n",
        "            indices_to_remove[index1].add(index2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y5oD9tBJdns_"
      },
      "outputs": [],
      "source": [
        "del title_counts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bILy6SwyRHvB"
      },
      "source": [
        "Create a mapping from every title to every corresponding list of ingredients"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mK7zudRXdpv0"
      },
      "outputs": [],
      "source": [
        "title_to_ingredients = {}\n",
        "for index, row in tqdm(all_data.iterrows(), total=len(all_data)):\n",
        "    title = row[\"title\"]\n",
        "    ingredients = \" \".join(eval(row[\"ingredients\"]))\n",
        "    if title in title_to_ingredients:\n",
        "        title_to_ingredients[title].append(ingredients)\n",
        "    else:\n",
        "        title_to_ingredients[title] = [ingredients]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y5R4rp3YdtIn"
      },
      "outputs": [],
      "source": [
        "del all_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kt_fk-wgRRX9"
      },
      "source": [
        "Create two lists containing recipe tuples. Each tuple has two components: a string containing a recipe title, and another string containing a second recipe title and a list of the ingredients used in that recipe. similar_title_pairs contains tuples with strings that are similar to each other, and different_title_pairs contains tuples that are different from each other"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GzQIKywBdvIQ"
      },
      "outputs": [],
      "source": [
        "similar_title_pairs = []\n",
        "different_title_pairs = []\n",
        "for bad_indices, sample in tqdm(zip(indices_to_remove, samples), total=len(samples)):\n",
        "    titles = sample[\"title\"].to_list()\n",
        "    ingredients = [\"\"]*len(sample)\n",
        "    for index, title in enumerate(titles):\n",
        "        if title in title_to_ingredients:\n",
        "            ingredients[index] = title_to_ingredients[title].pop()\n",
        "            if len(title_to_ingredients[title]) == 0:\n",
        "                title_to_ingredients.pop(title)\n",
        "        else:\n",
        "            bad_indices.add(index)\n",
        "    for index, row in sample.iterrows():\n",
        "        if index in bad_indices:\n",
        "            continue\n",
        "        sim_and_diff_links_str = row[\"similar_and_different_links\"]\n",
        "        cur_title = titles[index]\n",
        "        similar_links = eval(sim_and_diff_links_str[7:sim_and_diff_links_str.index(\"]\", 1) + 1])\n",
        "        cur_sim_pairs = [(cur_title, titles[sim_index] + \" \" + ingredients[sim_index]) for sim_index in similar_links if sim_index not in bad_indices]\n",
        "        similar_title_pairs.extend(cur_sim_pairs)\n",
        "        different_links = eval(sim_and_diff_links_str[sim_and_diff_links_str.index(\", a\") + 8:-2])\n",
        "        cur_diff_pairs = [(cur_title, titles[diff_index] + \" \" + ingredients[diff_index]) for diff_index in different_links if diff_index not in bad_indices]\n",
        "        different_title_pairs.extend(cur_diff_pairs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KK_0ck0Gdwxl"
      },
      "outputs": [],
      "source": [
        "del samples\n",
        "del title_to_ingredients\n",
        "del indices_to_remove"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hu7vdindRsFS"
      },
      "source": [
        "Create a Hugging Face Dataset using the data from similar_title_pairs and different_title_pairs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kNDf6Jwbd1Mu"
      },
      "outputs": [],
      "source": [
        "anchors = [pair[0] for pair in similar_title_pairs] + [pair[0] for pair in different_title_pairs]\n",
        "pos_neg = [pair[1] for pair in similar_title_pairs] + [pair[1] for pair in different_title_pairs]\n",
        "sim_length = len(similar_title_pairs)\n",
        "diff_length = len(different_title_pairs)\n",
        "del similar_title_pairs\n",
        "del different_title_pairs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4GyRBs92d3Pa"
      },
      "outputs": [],
      "source": [
        "dataset_dict = {\n",
        "    \"anchor\": anchors,\n",
        "    \"positive/negative\": pos_neg,\n",
        "    \"label\": [1]*sim_length + [0]*diff_length\n",
        "}\n",
        "del anchors\n",
        "del pos_neg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oaw_Pd6Cd5DP"
      },
      "outputs": [],
      "source": [
        "dataset = Dataset.from_dict(dataset_dict)\n",
        "del dataset_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6R-F45SuR0QH"
      },
      "source": [
        "Save dataset to disk so it can be read later"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0AatRH7Td64y"
      },
      "outputs": [],
      "source": [
        "dataset.save_to_disk(DATASET_LOCAL_DISK_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fb-OEDSCSCYF"
      },
      "source": [
        "# Prepare Training Variables\n",
        "Creates all the variables necessary to train the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BMnRonLfR7bl"
      },
      "source": [
        "Reads dataset from disk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GNtpCz65wuRc"
      },
      "outputs": [],
      "source": [
        "dataset = Dataset.load_from_disk(DATASET_LOCAL_DISK_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eW3SO9xWSdkh"
      },
      "source": [
        "If you are using CosineSimilarityLoss, you should only use positive examples. Therefore, we remove all examples where label == 0. We also rename the first two columns to sentence_A, and sentence_B. If you are using ContrastiveLoss, these changes aren't necessary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KxkgVGeC1QJO"
      },
      "outputs": [],
      "source": [
        "# Filters out negative pairs\n",
        "dataset = dataset.filter(lambda example: example[\"label\"] == 1)\n",
        "\n",
        "# Renames columns to sentence_A, sentence_B\n",
        "dataset = dataset.rename_columns({\"anchor\": \"sentence_A\", \"positive/negative\": \"sentence_B\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gX6enBMeTGZO"
      },
      "source": [
        "Split dataset into train data, validation data, and test data, which contain 49%, 21%, and 30% of the data, respectively"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t68D-JTyyq8O"
      },
      "outputs": [],
      "source": [
        "trainvalid_test = dataset.train_test_split(test_size=0.3)\n",
        "train_valid = trainvalid_test['train'].train_test_split(test_size=0.3)\n",
        "train_dataset = train_valid['train']\n",
        "valid_dataset = train_valid['test']\n",
        "test_dataset = trainvalid_test['test']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8BRP8RwuzAsF"
      },
      "outputs": [],
      "source": [
        "print(train_dataset)\n",
        "print(valid_dataset)\n",
        "print(test_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oh3L0Q-fTUjA"
      },
      "source": [
        "Gets tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ya7W0Z7SsRtf"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_ID)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3DxkIE3TYqc"
      },
      "source": [
        "Creates model and modified the model to use [LoRA](https://arxiv.org/abs/2106.09685). We do this by freezing the parameters in the original model, and adding new, low-rank matrices to a few of the weight layers in the model. We then fine-tune the parameters in these new low-rank matrices instead of the parameters in the original model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_lCvZa9EykRA"
      },
      "outputs": [],
      "source": [
        "model = SentenceTransformer(BASE_MODEL_ID, device=\"cuda\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BBwFsoQ4tUBK"
      },
      "outputs": [],
      "source": [
        "for param in model.parameters():\n",
        "  param.requires_grad = False\n",
        "  if param.ndim == 1:\n",
        "    param.data = param.data.to(torch.float32)\n",
        "config = LoraConfig(target_modules=[\"dense\"])\n",
        "model = get_peft_model(model, config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4R771uuUNc0"
      },
      "source": [
        "Create a loss function. This can be either ContrastiveLoss or CosineSimilarityLoss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CssyjYW81g3W"
      },
      "outputs": [],
      "source": [
        "loss = CosineSimilarityLoss(model=model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbZQgdSGUVW7"
      },
      "source": [
        "Login to Hugging Face using a token with write access. This allows us to push the model to Hugging Face Hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uaQ62TGQie8x"
      },
      "outputs": [],
      "source": [
        "notebook_login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTBspdbrUyFs"
      },
      "source": [
        "# Create Trainer and Train Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gh2F1BHJUfAf"
      },
      "source": [
        "Creates training arguments and trainer for the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xXNkzXIqd8kz"
      },
      "outputs": [],
      "source": [
        "args = SentenceTransformerTrainingArguments(\n",
        "    # Required parameter:\n",
        "    output_dir=\"all-mpnet-base-v2\",\n",
        "    # Optional training parameters:\n",
        "    max_steps=200_000,\n",
        "    per_device_train_batch_size=48,\n",
        "    per_device_eval_batch_size=48,\n",
        "    fp16=True,  # Set to False if your GPU can't handle FP16\n",
        "    bf16=False,  # Set to True if your GPU supports BF16\n",
        "    # Optional tracking/debugging parameters:\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=20_000,\n",
        "    prediction_loss_only=True,\n",
        "    save_strategy=\"no\",\n",
        "    save_steps=20_000,\n",
        "    logging_steps=20_000,\n",
        "    logging_strategy=\"steps\",\n",
        "    logging_first_step=True,\n",
        "    learning_rate=1e-6,\n",
        "    save_total_limit=10,\n",
        "    report_to=[\"none\"],\n",
        "    push_to_hub=True,\n",
        "    hub_strategy=\"every_save\",\n",
        "    hub_model_id=HF_MODEL_ID,\n",
        "    hub_private_repo=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hrF0D1efJJWy"
      },
      "outputs": [],
      "source": [
        "trainer = SentenceTransformerTrainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=valid_dataset.train_test_split(test_size=0.01)[\"test\"],\n",
        "    tokenizer=tokenizer,\n",
        "    loss=loss,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZM9Ykq5UmXp"
      },
      "source": [
        "Trains model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vONnKTfV2IQ0"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmkKnhPJL-qi"
      },
      "source": [
        "Push final model to Hugging Face Hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EORVNVkQDJJv"
      },
      "outputs": [],
      "source": [
        "model.push_to_hub(HF_MODEL_ID)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xL4ItpUaUqMv"
      },
      "source": [
        "# Loading model example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyWRQVPIL4BQ"
      },
      "source": [
        "Load model from Hugging Face Hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EbkrPNEz0GE_"
      },
      "outputs": [],
      "source": [
        "model = PeftModel.from_pretrained(SentenceTransformer(BASE_MODEL_ID, device=\"cuda\"), HF_MODEL_ID)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
